{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggMclvdIuN3J"
      },
      "outputs": [],
      "source": [
        "import sys, random, nltk\n",
        "from nltk import bigrams\n",
        "\n",
        "selected_features = None\n",
        "\n",
        "stopwords = ['all', 'just', 'being', 'over', 'both', 'through', 'yourselves', 'its', 'before', 'herself', 'had', 'should', 'to', 'only', 'under', 'ours', 'has', 'do', 'them', 'his', 'very', 'they', 'not', 'during', 'now', 'him', 'nor', 'did', 'this', 'she', 'each', 'further', 'where', 'few', 'because', 'doing', 'some', 'are', 'our', 'ourselves', 'out', 'what', 'for', 'while', 'does', 'above', 'between', 't', 'be', 'we', 'who', 'were', 'here', 'hers', 'by', 'on', 'about', 'of', 'against', 's', 'or', 'own', 'into', 'yourself', 'down', 'your', 'from', 'her', 'their', 'there', 'been', 'whom', 'too', 'themselves', 'was', 'until', 'more', 'himself', 'that', 'but', 'don', 'with', 'than', 'those', 'he', 'me', 'myself', 'these', 'up', 'will', 'below', 'can', 'theirs', 'my', 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', 'at', 'have', 'in', 'any', 'if', 'again', 'no', 'when', 'same', 'how', 'other', 'which', 'you', 'after', 'most', 'such', 'why', 'a', 'off', 'i', 'yours', 'so', 'the', 'having', 'once']\n",
        "\n",
        "def add_lexical_features(fdist, feature_vector, text):\n",
        "    feature_vector[\"len\"] = len(text)\n",
        "    text_nl = nltk.Text(text)\n",
        "    for word, freq in fdist.items():\n",
        "        fname = \"UNI_\" + word\n",
        "        if selected_features == None or fname in selected_features:\n",
        "            #feature_vector[fname] = text_nl.count(word)\n",
        "            feature_vector[fname] = 1\n",
        "\n",
        "def features(review_words):\n",
        "    feature_vector = {}\n",
        "\n",
        "    uni_dist = nltk.FreqDist(review_words)\n",
        "    my_bigrams = list(bigrams(review_words))\n",
        "    bi_dist = nltk.FreqDist(my_bigrams)\n",
        "\n",
        "    add_lexical_features(uni_dist,feature_vector, review_words)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "#to train the model\n",
        "#https://s3.amazonaws.com/hr-testcases/597/assets/trainingdata.txt\n",
        "file = open(\"trainingdata.txt\", \"rb\")\n",
        "raw_data = file.read().decode(\"latin1\")\n",
        "file.close()\n",
        "\n",
        "docs = raw_data.split(\"\\n\")\n",
        "docs2 = docs[1: ]\n",
        "train = []\n",
        "for d in docs2:\n",
        "    d = d.split()\n",
        "    if len(d)!=0:\n",
        "        cl = d[0]\n",
        "        text_d = d[1: ]#we need to remove the stopwords\n",
        "        text = []\n",
        "        for w in text_d:\n",
        "            if w not in stopwords:\n",
        "                text.append(w)\n",
        "        item = (text, cl)\n",
        "        train.append(item)\n",
        "\n",
        "random.seed(0)\n",
        "random.shuffle(train)\n",
        "\n",
        "train_set = train[ :4485]\n",
        "valid_set = train[4486: ]\n",
        "\n",
        "featuresets_tr = [(features(words), label) for (words, label) in train_set ]\n",
        "featuresets_val = [(features(words), label) for (words, label) in valid_set ]\n",
        "\n",
        "#featuresets = [(features(words), label) for (words, label) in train ]\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(featuresets_tr)\n",
        "\n",
        "#classifier = nltk.NaiveBayesClassifier.train(featuresets)\n",
        "\n",
        "#classifier.show_most_informative_features(50)\n",
        "accuracy = nltk.classify.accuracy(classifier, featuresets_val)\n",
        "#print(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#to take input\n",
        "n = int(input().strip())\n",
        "a = []\n",
        "for a_i in range(n): # to read a matrix\n",
        "    a_t = [a_temp for a_temp in input().strip().split(' ')]\n",
        "    a.append(a_t)\n",
        "\n",
        "featuresets_test = [features(words) for words in a ]\n",
        "\n",
        "predicted_labels = classifier.classify_many(featuresets_test)\n",
        "for l in predicted_labels:\n",
        "    print (int(l))"
      ]
    }
  ]
}